{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "179827af-581f-417c-aeb2-f193ad200c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import ndimage\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import warnings\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1172bfa9-75e5-4495-9555-7a16d0079d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, what needs to happen here?\n",
    "\n",
    "# Let's consider a single block\n",
    "    # - We have 20 time points (therefore images - just the horizontal ones)\n",
    "    # - Then we need to get the convolution maps \n",
    "    # - Next, we create the subwindows - 910 per image, and sum up the convolution to get the density\n",
    "    # - Each image will output subwindowed densities of shape (910, ) or (910, 1) - we might prefer the latter\n",
    "    # - Now since we have 20 images/convolved maps in the sequence, we will stack the final result to be of shape (910, 20, 1)\n",
    "    # - We can then store these np arrays and later stack these along with the feature maps to create the 910 csv files that will then be used for the BLAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a5bbf2-6d01-48a2-b0e9-e7eff6bfea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, let's start with the creation of density maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8abf000e-c634-4f30-bb04-1e9e5e53eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also do sanity checks to verify the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3a34bc-ad73-4ad8-bbff-df3475dbdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have previously written functions, let's use them here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0bbd2a-f740-43ec-9acd-3d09adb10b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the jpeg and xml files for which we need the density maps - basically all the horizontal images (20 of them) in the test blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717f3ece-2b95-46d2-bc7f-43cb87d46b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_xml_and_jpeg(file_location):\n",
    "    # list all files in location\n",
    "    list_of_all_files = os.listdir(file_location)\n",
    "    # sort files\n",
    "    list_of_all_files.sort()\n",
    "    # separate xml and jpeg files\n",
    "    all_xml_files = [file for file in list_of_all_files if file.split('.')[-1] == 'xml']\n",
    "    all_xml_files.sort()\n",
    "    all_jpeg_files = [file for file in list_of_all_files if file not in all_xml_files]\n",
    "    all_jpeg_files.sort()\n",
    "    # get the final 20 files\n",
    "    chosen_xml_files = all_xml_files[-20:]\n",
    "    chosen_jpeg_files = all_jpeg_files[-20:]\n",
    "    # make sure the xml and jpeg files correspond to each other?\n",
    "    mean = np.mean([file.split('.')[0] for file in chosen_xml_files] == [file.split('.')[0] for file in chosen_jpeg_files])\n",
    "\n",
    "    # chose the required files only - notice that for the inputs we do not need to create their density maps - therefore we do not need the xml files\n",
    "    task_specific_image_files = chosen_jpeg_files[-7:]\n",
    "    task_specific_xml_files =chosen_xml_files[-7:]\n",
    "    return(chosen_jpeg_files, chosen_xml_files, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5abf1530-ef8c-4b73-b948-6bc66cba5277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the convolved density maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9382706-7976-4436-b412-d537dd0fbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for horizontally annotated images\n",
    "def get_density_maps_horizontal(file_name, image_path, xml_path):\n",
    "    xml_file = file_name + '.xml'\n",
    "    xml_file_path = os.path.join(xml_path, xml_file)\n",
    "\n",
    "    # Get coords from the xml file\n",
    "    # parse the xml file\n",
    "    parsed_file = ET.parse(xml_file_path)\n",
    "    # get the roots\n",
    "    root = parsed_file.getroot()\n",
    "    # get the roots here\n",
    "    coords = []\n",
    "    for child in root:\n",
    "        for i in child:\n",
    "            for j in i:\n",
    "                coords.append(int(j.text))\n",
    "    \n",
    "    # chunk the points into sets of 4 - these are the coordinates of the bounding boxes\n",
    "    points_tupples = []\n",
    "    for i in range(0, len(coords), 4):\n",
    "        points_tupples.append(coords[i:i + 4])\n",
    "\n",
    "    # make a dataframe with these points\n",
    "    coords_df = pd.DataFrame(points_tupples, columns = [\"bleft_x\", \"bleft_y\", \"tright_x\", \"tright_y\"])\n",
    "\n",
    "    # compute the number of tassels in each image\n",
    "    no_of_tassels = len(points_tupples)\n",
    "\n",
    "    # compute the mid coordinates\n",
    "    coords_df[\"mid_x\"] = (round(0.5*(coords_df[\"bleft_x\"] + coords_df[\"tright_x\"]))).astype(int)\n",
    "    coords_df[\"mid_y\"] = (round(0.5*(coords_df[\"bleft_y\"] + coords_df[\"tright_y\"]))).astype(int)\n",
    "\n",
    "    # extract the mid cordinates\n",
    "    mid_coords = coords_df[[\"mid_x\", \"mid_y\"]]\n",
    "    # cap the coords at the max height and width values\n",
    "    mid_coords.loc[mid_coords['mid_x'] > 1024, 'mid_x'] = 1023\n",
    "    mid_coords.loc[mid_coords['mid_y'] > 768, 'mid_y'] = 767\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # plot the bounding boxes on images\n",
    "    # get image name and path\n",
    "    image_name = file_name + '.jpeg'\n",
    "    imge_file_path = os.path.join(image_path, image_name)\n",
    "    # read the image\n",
    "    read_image = plt.imread(imge_file_path)\n",
    "    read_image = ndimage.rotate(read_image, 0)\n",
    "\n",
    "    # check the shape of the read image\n",
    "    read_image_shape = read_image.shape\n",
    "    #  plot the bounding boxes on the image\n",
    "    for points in points_tupples:\n",
    "        annotated_image = cv2.rectangle(read_image, (points[0],points[1]), (points[2],points[3]), color = (255,0,0), thickness = 2)\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.show()\n",
    "\n",
    "    # plot the mid points on the image\n",
    "    coords_list = mid_coords.values.tolist()\n",
    "    # read the image again\n",
    "    read_image_again = plt.imread(imge_file_path)\n",
    "    read_image_again = ndimage.rotate(read_image_again, 0)\n",
    "    # draw the circles on image\n",
    "    for i in coords_list:\n",
    "        image_with_mids = cv2.circle(read_image_again, i, radius=5, color=(255, 0, 0), thickness=-1)\n",
    "    # look at the annotated image\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(image_with_mids)\n",
    "    plt.show()\n",
    "\n",
    "    # also try creating the density map here\n",
    "    # first create the empty maps\n",
    "    np_image = np.zeros((read_image_shape[0], read_image_shape[1]))\n",
    "    # get the dot maps\n",
    "    for point in coords_list:\n",
    "        np_image[point[1], point[0]] = 1\n",
    "    # plot the image\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(np_image, cmap = \"Greys\")\n",
    "    plt.show()\n",
    "\n",
    "    # now define the kernel and run the convolution\n",
    "    one_d_kerenel = cv2.getGaussianKernel(50,5)\n",
    "    two_d_kernel = np.multiply(one_d_kerenel.T, one_d_kerenel)\n",
    "\n",
    "    # Shape of the 2D kernel\n",
    "    twoD_shape = two_d_kernel.shape\n",
    "        \n",
    "    # do the convolution\n",
    "    convolution = ndimage.convolve(np_image, two_d_kernel)\n",
    "        \n",
    "    # plot the density map\n",
    "    # plt.figure(figsize = (12,18))\n",
    "    plt.imshow(convolution, cmap = \"Greys\")\n",
    "    plt.show()\n",
    "        \n",
    "    # get the sums of the images\n",
    "    img_sum = np.sum(convolution)\n",
    "\n",
    "    return(file_name, read_image_shape, no_of_tassels, img_sum, convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "368d432c-cc20-4476-ad9e-6151888ff87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the no_of_tassels and img_sum will give us the same numbers - and we may need to store these for future use as the true densities - maybe these will be needed for the inference in the BLAR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "461606e3-b511-4bc5-acef-f2613dac98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sub-window-wise densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c54f94f-336d-4d1a-b697-3c7ecd948d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_densities_for_subwindows(density_map, stride = 30, kernel_size = 30):\n",
    "        \n",
    "    # create the counts for the subwindows as follows\n",
    "    img_height = density_map.shape[0]\n",
    "    img_width = density_map.shape[1]\n",
    "    \n",
    "    density_sums = []\n",
    "    for i in  range(0, img_height, stride):\n",
    "        for j in range(0, img_width, stride):\n",
    "            # sub_window = loaded_im_file[i: i + kernel_size, j : j + kernel_size,:]\n",
    "            density = density_map[i: i + kernel_size, j : j + kernel_size]\n",
    "            dense_sum = np.sum(density)\n",
    "            density_sums.append(dense_sum)\n",
    "\n",
    "    print(\"sum of the convolved map: \", np.sum(density_sums))\n",
    "    return density_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb610e06-76ea-4e07-bdd3-034c4f8f6d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nrdstor_tfp_for_TN)",
   "language": "python",
   "name": "nrdstor_tfp_for_tn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
