{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7265f30c-277c-49a6-9aff-13b13ad860db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbf802b-2b57-4fb6-bf2d-b24c4b7c6296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74a73f-7605-42eb-a4e2-d171add05c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "root = tfd.JointDistributionCoroutine.Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fba023-2fa0-49ad-a6bf-86288a6bdcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test data\n",
    "\n",
    "def split_train_test_data(sub_image_df, path_to_df, n_forecasting):\n",
    "    # join the paths\n",
    "    complete_path_to_df = os.path.join(path_to_df, sub_image_df)\n",
    "    # read the csv \n",
    "    read_df = pd.read_csv(complete_path_to_df)\n",
    "\n",
    "    # split the data into train and test\n",
    "    train_df = read_df.iloc[:-n_forecasting, :]\n",
    "    print(train_df.shape)\n",
    "    test_df = read_df.iloc[-n_forecasting:,:]\n",
    "    print(test_df.shape)\n",
    "\n",
    "    # get the obs data\n",
    "    train_y = np.log(train_df['density'] + 1)\n",
    "    test_y = np.log(test_df['density'] + 1)\n",
    "\n",
    "    # make these float 32 for bayes ts implementation\n",
    "    train_y = train_y.astype(\"float32\")\n",
    "    test_y = test_y.astype(\"float32\")\n",
    "\n",
    "    # these needs to be returned\n",
    "\n",
    "    # also split the covariate data\n",
    "    # but add an intercept before the split?\n",
    "    read_df.insert(0, 'intercept', np.repeat(1, read_df.shape[0]))\n",
    "    # make this float32 for bayes ts implementation\n",
    "    read_df['intercept'] = read_df['intercept'].astype(\"float32\")\n",
    "\n",
    "    # now can extract the covariate data\n",
    "    X_preds = read_df.drop(['density'], axis = 1).astype(\"float32\")\n",
    "    X_preds = X_preds.values\n",
    "    print(X_preds.shape)\n",
    "    n_preds = X_preds.shape[-1]\n",
    "    return(train_y, test_y, X_preds, n_preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf00aba-2d65-48e8-9275-3e39f5cbbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the plot function\n",
    "def plot_tassel_count_data(train_data, test_data, df_no, fig, ax):\n",
    "    # if not fig_ax:\n",
    "    #     fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    # else:\n",
    "    #     fig, ax = fig_ax\n",
    "    ax.plot(train_data, color = 'blue', label=\"training data\")\n",
    "    ax.plot(test_data, color = 'lightcoral', label=\"testing data\")\n",
    "    ax.legend()\n",
    "    ax.set(\n",
    "        ylabel=\"Tassel counts\" ,\n",
    "        xlabel=\"Time\",\n",
    "        title = \"Tassel count distribution for sub image \" + str(df_no)\n",
    "    )\n",
    "    fig.autofmt_xdate()\n",
    "    fig.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfadd1de-c366-47d8-a941-7ec6c0db1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prioirs_and_x_beta(X_pred, n_pred):\n",
    "    beta = yield root(tfd.Sample(\n",
    "        tfd.Normal(0., 1.),\n",
    "        sample_shape=n_pred,\n",
    "        name='beta'))\n",
    "    x_beta = tf.einsum('ij,...j->...i', X_pred, beta)\n",
    "\n",
    "    noise_sigma = yield root(tfd.HalfNormal(scale=0.1, name='noise_sigma'))\n",
    "\n",
    "    intercept_data = X_pred[:,0]\n",
    "\n",
    "    return (x_beta, intercept_data, noise_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8eda5d-97f3-4548-926d-0d6b6f8836ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tp = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6b40a-a33d-4a76-bade-b0528c038c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the number of train steps here - in the next function we use this parameter even though the function does not take this value as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4762610-07cb-403e-9f29-6d4b107d8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_ar_latent(preds_data, n_pred, training=True):\n",
    "\n",
    "    @tfd.JointDistributionCoroutine # Used when defining the join distribution - we define the joint distribution in the function below\n",
    "    def model_with_latent_ar():\n",
    "        # we get the x_beta, intercept and the priors for noise sigma from the function defined earlier\n",
    "        x_beta, intercept_data, noise_sigma = yield from get_prioirs_and_x_beta(preds_data, n_pred)\n",
    "\n",
    "        # define the latent AR component as before\n",
    "        # Latent AR(1)\n",
    "        # define the two priors for the parameters of the AR(1) process\n",
    "        ar_sigma = yield root(tfd.HalfNormal(0.1, name='ar_sigma'))\n",
    "        rho = yield root(tfd.Uniform(0, 1., name='rho'))\n",
    "\n",
    "        # define the AR function below\n",
    "        def ar_fun(y):\n",
    "            loc = tf.concat([tf.zeros_like(y[..., :1]), y[..., :-1]],\n",
    "                            axis=-1) * rho[..., None]\n",
    "            return tfd.Independent(\n",
    "                tfd.Normal(loc=loc, scale=ar_sigma[..., None]),\n",
    "                reinterpreted_batch_ndims=1)\n",
    "        temporal_error = yield tfd.Autoregressive(\n",
    "            distribution_fn=ar_fun,\n",
    "            sample0=tf.zeros_like(intercept_data),\n",
    "            num_steps=intercept_data.shape[-1],\n",
    "            name='temporal_error')\n",
    "\n",
    "        # Linear prediction - note we add the X_beta and the temporal error to get y_hat: and we need the part of if training as we input all data at once and we only want\n",
    "        # train data to be used for the model training\n",
    "        y_hat = x_beta + temporal_error\n",
    "        if training:\n",
    "            y_hat = y_hat[..., :train_tp]\n",
    "\n",
    "        # Likelihood\n",
    "        observed = yield tfd.Independent(\n",
    "            tfd.Normal(y_hat, noise_sigma[..., None]),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "            name='observed'\n",
    "        )\n",
    "\n",
    "\n",
    "    # not sure wy we are returning the defined model though\n",
    "    return model_with_latent_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86bd050-f02d-4058-b02d-6f634bccac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to plot the X_beta and temporal errors - note we are only choosing the last 100 values in each chain for the plot\n",
    "def preds_and_temoral_error(mcmc_samples_data, preds_data, n_total_time_points, nchains):\n",
    "    # plot components\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 7.5), sharex=True)\n",
    "\n",
    "    beta = mcmc_samples_data[0]\n",
    "    seasonality_posterior = tf.einsum('ij,...j->...i', preds_data, beta)\n",
    "    temporal_error = mcmc_samples_data[-1]\n",
    "\n",
    "    for i in range(nchains):\n",
    "        ax[0].plot(np.arange(n_total_time_points), seasonality_posterior[-100:, i, :].numpy().T, alpha=.05);\n",
    "        ax[1].plot(np.arange(n_total_time_points), temporal_error[-100:, i, :].numpy().T, alpha=.05);\n",
    "\n",
    "    ax[0].set_title('X_beta effect')\n",
    "    ax[1].set_title('Temporal error')\n",
    "    ax[1].set_xlabel(\"Day\")\n",
    "    fig.autofmt_xdate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129934b-47cd-4c0a-80b2-d73dfa44b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the forecated and actual values - some 250 forecasted values, their averages, and the true values at all time points.\n",
    "def forecasted_and_actual_values_plot(ppc_sample_data, train_counts, test_counts, df_no, fig, ax):\n",
    "    fitted_with_forecast = ppc_sample_data[-1].numpy()\n",
    "    \n",
    "    ax.plot(np.arange(20), fitted_with_forecast[:250, 0, :].T, color='gray', alpha=.1);\n",
    "    ax.plot(np.arange(20), fitted_with_forecast[:250, 1, :].T, color='gray', alpha=.1);\n",
    "    \n",
    "    plot_tassel_count_data(train_counts, test_counts, df_no, fig, ax)\n",
    "    average_forecast = np.mean(fitted_with_forecast, axis=(0, 1)).T\n",
    "    ax.plot(np.arange(20), average_forecast, ls='--', label='latent AR forecast', color = 'red', alpha=.5);\n",
    "    plt.xticks(np.arange(20))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a9777-8d1c-47f9-a611-199187670792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to get the posteriors and the trace plots with az - for all model parameters\n",
    "def get_nuts_values_and_posterior_plots(mcmc_samples_bts, sampler_stats_bts):\n",
    "    nuts_trace_ar_latent = az.from_dict(posterior={k:np.swapaxes(v.numpy(), 1, 0) for k, v in mcmc_samples_bts._asdict().items()},\n",
    "    sample_stats = {k:np.swapaxes(sampler_stats_bts[k], 1, 0)for k in [\"target_log_prob\", \"diverging\", \"accept_ratio\", \"n_steps\"]})\n",
    "\n",
    "    axes = az.plot_trace(nuts_trace_ar_latent, var_names=['beta', 'ar_sigma', 'rho', 'noise_sigma'], compact=True);\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return(nuts_trace_ar_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce07ee75-3f58-47d2-bb45-5bab5921bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns all posterior values (distributions) for model parameters\n",
    "def posterior_vals(nuts_model):\n",
    "    rho_values = nuts_model.posterior.rho\n",
    "    print(rho_values.shape)\n",
    "    ar_sigma_values = nuts_model.posterior.ar_sigma\n",
    "    print(ar_sigma_values.shape)\n",
    "    noise_sigma_values = nuts_model.posterior.noise_sigma\n",
    "    print(noise_sigma_values.shape)\n",
    "    beta_vals_all = nuts_model.posterior.beta\n",
    "    print(beta_vals_all.shape)\n",
    "\n",
    "    return(rho_values, ar_sigma_values, noise_sigma_values, beta_vals_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db4213-3e40-47e8-b92f-196758adb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for traceplots\n",
    "color_list = ['cornflowerblue', 'lightsteelblue', 'blue', 'mediumblue', 'cyan', 'deepskyblue', 'steelblue', 'dodgerblue', 'lightslategray', 'mediumslateblue',\n",
    "             'lightblue', 'teal', 'royalblue', 'indianred', 'deepskyblue', 'honeydew', 'lightseagreen', 'turquoise', 'cadetblue', 'tan', 'moccasin', 'burlywood',\n",
    "             'peachpuff', 'powderblue', 'mediumaquamarine', 'powderblue', 'thistle', 'lavender', 'lightcyan', 'darkseagreen', 'honeydew', 'lightsteelblue', 'cadetblue']\n",
    "len(color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59010f3-88b6-45de-ad81-43f8d0f0572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color palattes for freq polygons\n",
    "color_palletes_betas = ['Greys','Purples','Blues','Greens','Oranges','Reds','YlOrBr','YlOrRd','OrRd','PuRd','RdPu','BuPu','GnBu','PuBu','YlGnBu',\n",
    " 'PuBuGn','BuGn','YlGn','Greys','Purples','Blues','Greens','Oranges','Reds','YlOrBr','YlOrRd','OrRd','PuRd','RdPu','BuPu','GnBu','PuBu', 'YlGnBu']\n",
    "len(color_palletes_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f6cf9-9d5c-4c7d-a84e-3b084368915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow alogn from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05364483-7d17-4669-87a5-28c3fe248ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for creating both trace plots and freqeuncy polygons which are clearer, as the ones generated automatically are not very nice\n",
    "\n",
    "# for a chosen parameter - param\n",
    "\n",
    "def get_trace_plots(ax, param, color, string_params):\n",
    "    ax.plot(param.T, color = color, alpha = 0.5)\n",
    "    ax.set_title(\"Trace plot for \" + string_params, fontsize=10, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7ffed-c6ea-45bf-bea4-cc3f80e6dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for this?\n",
    "def get_freq_curves(ax, param, color_palette, string_params):\n",
    "    sns.kdeplot(data=param.T, fill=False, ax=ax, legend = False, palette = color_palette)\n",
    "    ax.set_title(\"Frequency plot for \" + string_params, fontsize=10, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23128a-4714-4274-b857-5c0e5a5ae8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, seems like all the work is happening in the below function, where everything is define above is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da91c9-3f44-44be-b9a7-6070d5dcefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_Bayes_TS_model(csv_file, forecasting_steps, path_to_preprocessed_dfs, sub_image_number, n_features, figure_folder_path, forecasts_folder_path, nchains):\n",
    "    \n",
    "    ############ We understand the code chunks below #############\n",
    "    # get the counts and predictors\n",
    "    Train_Y, Test_Y, X_preds_only, n_predictors = split_train_test_data(csv_file, path_to_preprocessed_dfs, forecasting_steps)\n",
    "    # plot the train and test counts\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    plot_tassel_count_data(Train_Y, Test_Y, sub_image_number, fig, ax)\n",
    "    # define the mcmc function - we will be using this in a bit - we are using the adaptive nuts HMC method for getting the posterior distributions and predictions\n",
    "    run_mcmc = tf.function(tfp.experimental.mcmc.windowed_adaptive_nuts , autograph=False, jit_compile=True)\n",
    "    # define the latet ar model\n",
    "    gam_with_latent_ar = generate_model_ar_latent(X_preds_only, n_predictors, training=True)\n",
    "    # plot samples from prior predictive distribution  - we haven't defined a function previously for this\n",
    "    plt.figure(figsize = (10,8))\n",
    "    plt.plot(tf.transpose(gam_with_latent_ar.sample(500)[-1]))\n",
    "    plt.show()\n",
    "    # run the mcmc with nuts sampler - run for 1000 more steps after a 1000 burning period\n",
    "    # mcmc_samples - this has all the posterior distributions for the parameters of the model - beta, rho, ar_sigma, noise_sigma and temporal errors\n",
    "    # sampler_stats - this carries variaous information about the NUTs sampler -  about convergenece steps etc\n",
    "    mcmc_samples, sampler_stats = run_mcmc(1000, gam_with_latent_ar, n_chains=4, num_adaptation_steps=1000, seed=tf.constant([36245, 734565], dtype=tf.int32), observed=Train_Y.T)\n",
    "    # get the posterior values for parameters and the posterioir predictions\n",
    "\n",
    "    # Why we do this again - maybe as we would like to get predictions for the entire time period and not just the test dataset (The predicted values - for the targets)\n",
    "    gam_with_latent_ar_full = generate_model_ar_latent(X_preds_only, n_predictors, training=False)\n",
    "    # below, we get the posterior_dists - which gives us meta data about what is stored in the ppc_samples variable\n",
    "    # ppc_samples - This gives us the posterior distributions for the model parameters (same as mcmc_samples), and also teh posterior predictions for the target. The last array of this\n",
    "    # tupple are the posterior predictions\n",
    "    posterior_dists, ppc_samples = gam_with_latent_ar_full.sample_distributions(value=mcmc_samples)\n",
    "    # plot the posteriors ? for betas and temporal errors - here we plot X_beta, and the temporal error\n",
    "    preds_and_temoral_error(mcmc_samples, X_preds_only, n_total_time_points = 20, nchains = 4)\n",
    "    \n",
    "    # Plot the forecated and actual values - for all time points\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    # fig_ax = (fig, ax)\n",
    "    \n",
    "    forecasted_and_actual_values_plot(ppc_samples, Train_Y, Test_Y, sub_image_number, fig, ax)\n",
    "    # Get the posterior plots -  the trace plots and the polygons with arviz - but they are not very good looking\n",
    "    nuts_output = get_nuts_values_and_posterior_plots(mcmc_samples, sampler_stats)\n",
    "\n",
    "    # recreate this plot with the functions written later to get clearer plots\n",
    "    rho_values, ar_sigma_values, noise_sigma_values, beta_vals_all = posterior_vals(nuts_output)\n",
    "    # get the betas gathered and plot them all in a single plot\n",
    "    all_betas = [beta_vals_all[: ,: , i] for i in range(n_features)]\n",
    "    # recreate the plot\n",
    "    fig, axs = plt.subplots(4, 2, figsize=(10, 10))\n",
    "    get_trace_plots(axs[0, 0], rho_values, 'palevioletred', 'rho')\n",
    "    get_trace_plots(axs[1, 0], noise_sigma_values, 'blue', 'noise sigma')\n",
    "    get_trace_plots(axs[2, 0], ar_sigma_values, 'indianred', 'ar sigma')\n",
    "    for i in range(33):\n",
    "        get_trace_plots(axs[3, 0], all_betas[i], color_list[i], 'betas')\n",
    "    fig.tight_layout()\n",
    "    get_freq_curves(axs[0, 1], rho_values, 'Blues', 'rho')\n",
    "    get_freq_curves(axs[1, 1], noise_sigma_values, 'Purples', 'noise sigma')\n",
    "    get_freq_curves(axs[2, 1], ar_sigma_values, 'Greens', 'ar sigma')\n",
    "    for i in range(33):\n",
    "        get_freq_curves(axs[3, 1], all_betas[i], color_palletes_betas[i], 'betas')\n",
    "    fig.tight_layout()\n",
    "    # plt.plot(all_betas[i].T, color = color_list[i], alpha = 0.5)\n",
    "    figure_path_and_name = figure_folder_path + '/' + 'all_trace_plots_sub_' + str(sub_image_number) + '.png'\n",
    "    plt.savefig(figure_path_and_name)\n",
    "    plt.show()\n",
    "    ############ we understand what is going on in the above chunk of codes ############\n",
    "\n",
    "    \n",
    "    # get the forecasted values\n",
    "    # Predicted values using the ppc_samples - last dimenison return these values\n",
    "    forecasted_values = ppc_samples[-1].numpy() # this will be of shape (1000,4,20)\n",
    "    averaged_forecast = np.mean(forecasted_values, axis=(0, 1)).T # this will be of shape (20,)\n",
    "    print(forecasted_values.shape, averaged_forecast.shape)\n",
    "    \n",
    "    # we may need to store the averaged forecasts - extract these first for the test set only\n",
    "    test_averaged_forecast = averaged_forecast[-forecasting_steps:]\n",
    "    test_all_forecasts = forecasted_values[:,:,-forecasting_steps:]\n",
    "    \n",
    "    # create a dataframe for true and averaged forecasts for test data\n",
    "    final_forecasted_values = pd.DataFrame(zip(Test_Y, test_averaged_forecast), columns = ['True_value', 'Forecasted_value'])\n",
    "    \n",
    "    # save the avrage and all forecasts for future use\n",
    "    avg_frcst_file_name = forecasts_folder_path + '/' + 'averaged_forecasts_sub_' + str(sub_image_number) + '.csv'\n",
    "    final_forecasted_values.to_csv(avg_frcst_file_name, index = False)\n",
    "    all_frcst_file_name = forecasts_folder_path + '/' + 'all_forecasts_sub_' + str(sub_image_number) + '.npy'\n",
    "    np.save(all_frcst_file_name, test_all_forecasts)\n",
    "    \n",
    "    # get the parameter summary\n",
    "    parameter_summary = az.summary(nuts_output)\n",
    "    # This will give all information about the posterior distributions in a df, save these if we need in future\n",
    "    parameter_summary_file_name = forecasts_folder_path + '/' + 'posterior_parameter_summary_sub_' + str(sub_image_number) + '.csv'\n",
    "    parameter_summary.to_csv(parameter_summary_file_name, index = False)\n",
    "    \n",
    "    # save the posterior distributions for other parameters\n",
    "    rho_file_name = forecasts_folder_path + '/' + 'rho_values_sub_' + str(sub_image_number) + '.npy'\n",
    "    np.save(rho_file_name, rho_values)\n",
    "    noise_sigma_file_name = forecasts_folder_path + '/' + 'noise_sigma_values_sub_' + str(sub_image_number) + '.npy'\n",
    "    np.save(noise_sigma_file_name, noise_sigma_values)\n",
    "    ar_sigma_file_name = forecasts_folder_path + '/' + 'ar_sigma_values_sub_' + str(sub_image_number) + '.npy'\n",
    "    np.save(ar_sigma_file_name, ar_sigma_values)\n",
    "    beta_file_name = forecasts_folder_path + '/' + 'beta_values_sub_' + str(sub_image_number) + '.npy'\n",
    "    np.save(beta_file_name, all_betas)\n",
    "    \n",
    "    return(final_forecasted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c453494-17da-4984-b168-1aa356a43ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need changes to paths from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751bde40-f54d-4742-b5b4-b3747844c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_name = 'extracted_features_sub_window_1.csv'\n",
    "forecasting_steps = 7\n",
    "path_to_precessed_dfs = 'data/BLAR_ready_dfs/model_1/block_0103/'\n",
    "# sub_image_number = 0\n",
    "n_features = 33\n",
    "nchains = 4\n",
    "figure_folder_path = 'data/BLAR_implementation/Block_0103/figures'\n",
    "forecasts_folder_path = 'data/BLAR_implementation/Block_0103/forecasted_counts'\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "# fig_ax = (fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7791e-ce41-4638-b45b-0478901b2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get rid of the checkpointing folder\n",
    "# sub_image_files = [file for file in os.listdir(path_to_precessed_dfs) if file[-3:] == 'csv']\n",
    "# sub_image_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d4c79-128e-4fc1-acc9-939f721e30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_image_files = ['block_0103_df_' + str(i) +'.csv' for i in range(910)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0698583-2c18-415b-a51d-cb788bb6b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ba55b-99c0-45a3-a601-f1a8462b1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_image_numbers = np.arange(910)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b3325-5a46-4e5f-bb43-af80c5fb44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_image_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde653d-b431-4ddc-9270-e953d555a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ef216-de42-400b-8866-270e96aab726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(i):\n",
    "    return fit_Bayes_TS_model(\n",
    "        sub_image_files[i], forecasting_steps, path_to_precessed_dfs,\n",
    "        sub_image_numbers[i], n_features, figure_folder_path,\n",
    "        forecasts_folder_path, nchains\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd2908-fb30-419b-adca-1a54648f3ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=6) as executor:\n",
    "    all_dfs = list(tqdm(executor.map(run_model, range(50)), total = 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32f70d08-3967-4142-99a7-42db7b6deb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the code for the rest of the dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124d5d01-3ffc-4da3-9675-1bc813f7e0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 60):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0c9a23-c4b5-4d2b-a2e3-e9e789445718",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_image_files_1 = ['block_0103_df_' + str(i) +'.csv' for i in range(100, 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed989d2-91f5-46da-a4df-a21a41498fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('block_0103_df_100.csv', 'block_0103_df_199.csv')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_image_files_1[0], sub_image_files_1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b51b624a-57f7-4555-b545-d4dd2c49ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_image_numbers = np.arange(100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6208e532-2520-4983-b6fa-fb1a3f34af31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 199)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_image_numbers[0], sub_image_numbers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4110aba1-08bd-43e5-b0ae-78f790d6195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_image_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398a434-9036-4507-ac5a-ec8503aa45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to figureout what all is happening here - does not seem like we have what we need at the moment. Should we be chunking up this code? Maybe that's a better way to deal with this model. Also, for some plots it's very eveident the fitted model is not converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eff602-60aa-47f8-bb99-1f316b263e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try submitting this as a job, seem to be some issue with the parallel processing, and we need to figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5c967-3be0-4aed-8cc6-a5014adf3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to make sure we are in fact correctly doing the BLAR model - please check on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7aed6e-f65e-4ea0-9693-c1551552a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems we are doing a sanity check below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1641a79-bd65-4bb9-b3a6-535119da3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the average forecasted values match with all forecasts we stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92703beb-fe77-446f-88d6-82d612297a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to stored forecasts\n",
    "path_to_stored_forecasts = 'data/BLAR_implementation/Block_0103/forecasted_counts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5ad1886-cc68-45fe-bbd2-1f08b058f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the contents here\n",
    "contents = os.listdir(path_to_stored_forecasts)\n",
    "contents.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad4f3f42-0ddf-4a7b-8402-16d7241437bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the all and average forecasts\n",
    "all_forecasts_list = [file for file in contents if file[:3]=='all']\n",
    "all_forecasts_list.sort()\n",
    "avg_forecasts_list = [file for file in contents if file[:3] == 'ave']\n",
    "avg_forecasts_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0cb1dae-87c9-457f-a909-cc29ed748b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check we can get the values to match for the first sub-window, and later write a function to make sure we can do this for all sub - windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb3021b5-db96-4e4b-86f2-d264d4369d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the npy and the csv files\n",
    "all_forecasts_sub_0 = np.load(os.path.join(path_to_stored_forecasts, 'all_forecasts_sub_0.npy'))\n",
    "avg_forecast_sub_0 = pd.read_csv(os.path.join(path_to_stored_forecasts, \"averaged_forecasts_sub_0.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f00e47d-54eb-4593-ae9d-899ab71f2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_averages = avg_forecast_sub_0[['Forecasted_value']].values.flatten()\n",
    "# convert this to float 32\n",
    "stored_averages = np.float32(stored_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0648775b-08b9-49b6-9b6d-090c3cf5b049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00727865, -0.00744858, -0.00598132, -0.00544732, -0.00698081,\n",
       "       -0.00747231, -0.00311148], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d87cbfb6-f5a9-4974-854e-5363e8e3f8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_forecasts_sub_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c59528db-9c41-4955-8956-23374285c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_from_all_forecasts = all_forecasts_sub_0.mean(axis = (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad1c5cb4-dee0-4bdd-b255-bb61fee9b75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00727865, -0.00744858, -0.00598132, -0.00544732, -0.00698081,\n",
       "       -0.00747231, -0.00311148], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_from_all_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf177c0c-7b19-4377-bbb4-d0b51602f888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(stored_averages == avg_from_all_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "932b5a87-d254-418b-8862-98e608d7cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this to a function to test for all subwindows\n",
    "def match_averages(path_to_stored_forecasts, all_forecasts_file_name, average_forecasts_file_name):\n",
    "    # read in the npy files and the csv files\n",
    "    all_forecasts = np.load(os.path.join(path_to_stored_forecasts, all_forecasts_file_name))\n",
    "    avg_forecast = pd.read_csv(os.path.join(path_to_stored_forecasts, average_forecasts_file_name))\n",
    "    stored_average = avg_forecast[['Forecasted_value']].values.flatten()\n",
    "    # convert these to float32\n",
    "    stored_average = np.float32(stored_average)\n",
    "    # compute the means from all forecasts\n",
    "    avg_from_all_forecast = all_forecasts.mean(axis = (0,1))\n",
    "    print(np.mean(stored_average == avg_from_all_forecast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8939b64-1c90-4e02-a5a1-79f99e0fd90c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(0,910):\n",
    "    match_averages(path_to_stored_forecasts, all_forecasts_list[i], avg_forecasts_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b58c8-b87f-4ed9-bf56-b79bfdb3b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay the stored forecasts match with the computed averages from stored all forecasts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfp_cpu_env)",
   "language": "python",
   "name": "tfp_cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
